# Pytorch and Pytorch-lighning packages
import logging
import os

# Logging
import hydra
import pytorch_lightning as pl
import torch
from hydra.utils import to_absolute_path

# import from project
from model import Model
from omegaconf import OmegaConf
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
from google.cloud import storage
import wandb
from data import load_chest_xray_data, preprocess_data
from pathlib import Path


def set_seed(seed: int):
    """
    Sets the seed for reproducibility in PyTorch with GPUs and CPU
    Args:
        seed (int): specific value.
    """
    torch.manual_seed(seed)  # PyTorch CPU
    torch.cuda.manual_seed(seed)  # PyTorch GPU
    torch.cuda.manual_seed_all(seed)  # PyTorch multi GPUs
    torch.backends.cudnn.deterministic = True  # Make CUDA deterministic
    torch.backends.cudnn.benchmark = False  # Disable CUDA autotuning


log = logging.getLogger(__name__)


@hydra.main(config_path=to_absolute_path("configs/train_config"), config_name="default_config.yaml", version_base="1.1")
def train(config) -> None:
    raw_dir = to_absolute_path(os.path.join("data", "raw"))
    data_dir = to_absolute_path(os.path.join("data", "processed"))

    os.makedirs(data_dir, exist_ok=True)
    if not os.listdir(data_dir):
        preprocess_data(raw_dir, data_dir)

    print(f"configuration: \n {OmegaConf.to_yaml(config)}")
    hparams = config["experiment"]  # loading hyperparameters
    set_seed(hparams["seed"])  # setting reproducible seed
    # Wandb setup for project
    wandb.finish()
    wandb_logger = WandbLogger()
    wandb.init(
        project="X-Ray - Classification of Pneumonia",
        config={
            "lr": hparams["lr"],
            "weight_decay": hparams["wd"],
            "batch_size": hparams["batch_size"],
            "epochs": hparams["n_epochs"],
        },
    )

    # During hyperparameter sweeps, it needs to be updated to use the random
    # configuration generated by the sweep.
    hparams["lr"] = wandb.config.lr
    hparams["batch_size"] = wandb.config.batch_size
    hparams["n_epochs"] = wandb.config.epochs

    # Pytorch-lightning initialize model from model.py
    model = Model(model_name=hparams["model_name"], num_classes=1, lr=hparams["lr"], wd=hparams["wd"])
    print(f"Number of parameters: {sum(p.numel() for p in model.parameters())}")

    # For output logging
    print(
        f"lr = {hparams['lr']}, weight_decay = {hparams['wd']}, batch_size={hparams['batch_size']}, epochs={hparams['n_epochs']}"
    )
    wandb.config.update(
        {
            "lr": hparams["lr"],
            "weight_decay": hparams["wd"],
            "batch_size": hparams["batch_size"],
            "epochs": hparams["n_epochs"],
        }
    )

    # Using absolute path to ensure working dir is correct
    trainset, testset, valset = load_chest_xray_data(data_dir)

    # Dataloader for training and testing set
    train_dataloader = torch.utils.data.DataLoader(
        trainset,
        batch_size=hparams["batch_size"],
        shuffle=True,
        num_workers=hparams["num_workers"],
        persistent_workers=hparams["persistent_workers"],
    )
    test_dataloader = torch.utils.data.DataLoader(
        testset,
        batch_size=hparams["batch_size"],
        shuffle=False,
        num_workers=hparams["num_workers"],
        persistent_workers=hparams["persistent_workers"],
    )
    val_dataloader = torch.utils.data.DataLoader(
        valset,
        batch_size=hparams["batch_size"],
        shuffle=False,
        num_workers=hparams["num_workers"],
        persistent_workers=hparams["persistent_workers"],
    )

    model_save_path = to_absolute_path("models")
    os.makedirs(model_save_path, exist_ok=True)
    checkpoint_callback = ModelCheckpoint(
        dirpath=model_save_path,
        filename=f"trained_{hparams['model_name']}",
        save_top_k=1,  # Save only the best model
        verbose=True,
        monitor="val_loss",
        mode="min",  # Save when validation loss is minimized
        every_n_epochs=2,
    )
    # Trainer with WANDB logging
    trainer = pl.Trainer(
        max_epochs=hparams["n_epochs"],
        devices=1 if torch.cuda.is_available() else "auto",
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        logger=wandb_logger,
        callbacks=[checkpoint_callback],
        log_every_n_steps=20,
    )

    trainer.fit(model, train_dataloader, val_dataloader)
    trainer.test(model, test_dataloader)
    wandb.finish()

    def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):
        """
        Uploads a file to a GCS bucket.

        Args:
            bucket_name (str): Name of the GCS bucket.
            source_file_name (str): Path to the local file.
            destination_blob_name (str): Destination path in the GCS bucket.
        """
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)
        blob.upload_from_filename(source_file_name)
        print(f"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}.")

    def push_latest_model_to_gcs():
        # Find the most recent model checkpoint
        bucket_name = "trained_models_mlops"
        destination_prefix = "trained_models"
        model_save_path = Path(to_absolute_path("models"))
        # Find most recent file
        saved_files = sorted(
            Path(model_save_path).glob("trained_*.ckpt"), key=lambda f: f.stat().st_mtime, reverse=True
        )

        if saved_files:
            latest_model = saved_files[0]
            destination_blob_name = f"{destination_prefix}/{latest_model.name}"
            upload_to_gcs(bucket_name, str(latest_model), destination_blob_name)
        else:
            print("No model checkpoints found to upload.")

    push_latest_model_to_gcs()


if __name__ == "__main__":
    train()
